{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adversarial attacks and robustness for quantum machine learning\n===============================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This demo is based on the paper *A Comparative Analysis of Adversarial\nRobustness for Quantum and Classical Machine Learning Models* by M.\nWendlinger, K. Tscharke and P. Debus, which dives into the world of\nadversarial attacks on quantum computers to find relations to classical\nadversarial machine learning. In the following, we briefly cover the\nnecessary theoretical baselines of adversarial attacks on classification\nnetworks before giving a hands-on example of how to construct such an\nattack and how we can make QML models more robust. Let's go!\n\n![](../_static/demonstration_assets/adversarial_attacks_QML/QAML_overview.png){.align-center\nwidth=\"80.0%\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# What are adversarial attacks?\n\nAdversarial attacks are small, carefully crafted perturbations to input\ndata that lead to high rates of misclassification for machine learning\nmodels, while the data seems to be identical to the original for human\nobservers. In particular, images are sensitive to this type of attack,\nas small manipulations of the pixels can fool a classifier without any\nchanges being visible to humans. A typical example of an adversarial\nattack is shown in the picture above, where an image of a panda is\nmanipulated by adding a small noise to each pixel. The original image is\ncorrectly classified as \\\"Panda,\\\" while the image with tiny\nmanipulations is falsely classified as \\\"Gibbon\\\" (the noise is\nmagnified in the figure so we can actually see it). This example is\nadapted from a famous paper showing the vulnerability of classical\nmachine learning models to adversarial attacks.\n\nMathematically, the goal of an (untargeted) attack is to achieve a\nmisclassification of the model such that a sample $x$ leads to a\npredicted label $y' \\neq y$ that is not the true label $y.$ This is\nachieved by finding the perturbation $\\delta\\in\\Delta$ to the original\ninput that maximizes the loss of the true class. For a loss function\n$\\mathcal{L},$ a model $f: \\mathbb{R}^{D} \\to \\mathbb{R}^K$ (mapping\n$D$-dimensional input to\n[softmax](https://en.wikipedia.org/wiki/Softmax_function) probability\nscores of $K$ classes with model parameters $\\theta^*$), the objective\nof the untargeted attack is:\n\n$$\\delta \\equiv \\; \\underset{\\delta^{\\prime} \\in \\Delta}{\\operatorname{argmax}} \\;\\mathcal{L}\\left(f\\left(x+\\delta^{\\prime} ; \\theta^*\\right), y\\right).$$\n\nLater, when we show how to actually construct such an attack, we will\nrevisit this equation. For an adversarial attack to be considered\nuseful, it must hold that the modifications to the input elements are\nimperceptible, i.e.\u00a0that\n$\\Delta=\\{\\delta \\in \\mathbb{R}^{D}: \\| \\delta\\|_{\\infty} \\le \\varepsilon\\},$\nwhere $\\varepsilon$ is some small bound, typically below $0.1.$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Why are adversarial attacks dangerous?\n\nMachine learning (ML) is becoming essential in many security-critical\napplications, for example in\n[cybersecurity](https://arxiv.org/abs/1901.03407), the [medical\nsector](https://arxiv.org/abs/1606.05718), or [autonomous\nvehicles](https://arxiv.org/abs/1604.07316). In cybersecurity, ML models\nare used, amongst others, for malware detection or the prevention of\nattacks on networks. Given their critical roles, any successful attack\nof these ML models can lead to severe consequences, ranging from\nfinancial losses and privacy violations to threats to human life. As we\nhave seen above, adversarial attacks are imperceptible to humans, and\nhence difficult to detect. For this reason, it is essential that ML\nmodels in security-critical applications are robust against these types\nof attacks.\n\n[Quantum machine learning\n(QML)](%5B/whatisqml%5D(https://pennylane.ai/qml/whatisqml/)) has been\nshown to have theoretical advantages over classical ML methods and is\nbecoming increasingly popular. However, first works in this direction\nsuggest that QML suffers from the same vulnerabilities as classical ML.\nHow the vulnerability of QML models relates to classical models and how\nrobust the models are in comparison is evaluated in. But enough talk,\nlet's see how we can actually attack a QML model!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Let's see this in action!\n\n## Setting up the environment\n\nFor this tutorial, we will use the PennyLane\n`~pennylane.qnn.TorchLayer`{.interpreted-text role=\"class\"} class to\nperform circuit operations and optimizations with the PyTorch backend.\nThus, we need to import the torch library alongside PennyLane:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nfrom pennylane import numpy as np\nimport torch\nfrom matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualization of the dataset\n\nAs in the paper, we make use of the\n[PlusMinus](https://pennylane.ai/datasets/single-dataset/plus-minus)\ndataset (available via [PennyLane\nDatasets](https://pennylane.ai/datasets/)), which serves as a good\nbaseline for evaluating a QML image classification model's ability to\nfind useful features in the input. It also allows us to define the\nusefulness of attacks on the QML model while being low-dimensional\nenough to perform scalable training (more info on the dataset can be\nfound in). It consists of four classes of $16\\times16$ pixel grayscale\nimages which show one of the four symbols $\\{+,-,\\vdash,\\dashv\\}.$ Below\nwe visualize one sample of each class to get an understanding of the\ndataset.\n\nThe data can be loaded directly from [PennyLane\nDatasets](https://pennylane.ai/datasets/) for easy integration into\nPennyLane circuits and optimization code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# we can use the dataset hosted on PennyLane\n[pm] = qml.data.load('other', name='plus-minus')\n\nX_train = pm.img_train  # shape (1000,16,16)\nX_test = pm.img_test  # shape (200,16,16)\nY_train = pm.labels_train  # shape (1000,)\nY_test = pm.labels_test  # shape (200,)\n\n# show one image from each class (in the train- and testsets, the images are randomly permuted)\nx_vis = [\n    (X_train[Y_train == 0])[0],\n    (X_train[Y_train == 1])[0],\n    (X_train[Y_train == 2])[0],\n    (X_train[Y_train == 3])[0],\n]\ny_vis = [0, 1, 2, 3]\n\n\n# later when we train the model we include the predictions as well, so let's just add the functionality here\ndef visualize_data(x, y, pred=None):\n    n_img = len(x)\n    labels_list = [\"\\u2212\", \"\\u002b\", \"\\ua714\", \"\\u02e7\"]\n    fig, axes = plt.subplots(1, 4, figsize=(8, 2))\n    for i in range(n_img):\n        axes[i].imshow(x[i], cmap=\"gray\")\n        if pred is None:\n            axes[i].set_title(\"Label: {}\".format(labels_list[y[i]]))\n        else:\n            axes[i].set_title(\"Label: {}, Pred: {}\".format(labels_list[y[i]], labels_list[pred[i]]))\n    plt.tight_layout(w_pad=2)\n    # plt.show()\n\n\nvisualize_data(x_vis, y_vis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building the QML circuit for classification\n\nWe will make use of a\n`data-reuploading <tutorial_data_reuploading_classifier>`{.interpreted-text\nrole=\"doc\"} scheme to encode the 256 input pixels into the latent space\nof the quantum classifier. To this end, the\n`~pennylane.StronglyEntanglingLayers`{.interpreted-text role=\"class\"}\ntemplate provides an easy-to-use structure for the circuit design. The\noutput of our classifier is a four-dimensional vector resulting from\nPauli Z oberservables along the first four qubits. These outputs\n(unnormalized probability scores --- i.e.\u00a0logits) are then used in the\n[CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\nfunction to optimize the model parameters. Together with the PyTorch\nintegration mentioned above, the classifier code looks like the\nfollowing:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#### Hyperparameters ####\ninput_dim = 256\nnum_classes = 4\nnum_layers = 32\nnum_qubits = 8\nnum_reup = 3\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\nclass QML_classifier(torch.nn.Module):\n    \"\"\"\n    Class for creating a quantum machine learning (classification) model based on the StronglyEntanglingLayers template.\n\n    Args:\n        input_dim: the dimension of the input samples\n        output_dim: the dimension of the output, i.e. the numbers of classes \n        num_qubits: the number of qubits in the circuit\n        num_layers: the number of layers within the StronglyEntanglingLayers template\n    \"\"\"\n    def __init__(self, input_dim, output_dim, num_qubits, num_layers):\n        super().__init__()\n        torch.manual_seed(1337)  # fixed seed for reproducibility\n        self.num_qubits = num_qubits\n        self.output_dim = output_dim\n        self.num_layers = num_layers\n        self.device = qml.device(\"lightning.qubit\", wires=self.num_qubits)\n        self.weights_shape = qml.StronglyEntanglingLayers.shape(\n            n_layers=self.num_layers, n_wires=self.num_qubits\n        )\n\n        @qml.qnode(self.device)\n        def circuit(inputs, weights, bias):\n            inputs = torch.reshape(inputs, self.weights_shape)\n            qml.StronglyEntanglingLayers(\n                weights=weights * inputs + bias, wires=range(self.num_qubits)\n            )\n            return [qml.expval(qml.PauliZ(i)) for i in range(self.output_dim)]\n\n        param_shapes = {\"weights\": self.weights_shape, \"bias\": self.weights_shape}\n        init_vals = {\n            \"weights\": 0.1 * torch.rand(self.weights_shape),\n            \"bias\": 0.1 * torch.rand(self.weights_shape),\n        }\n\n        # initialize the quantum circuit\n        self.qcircuit = qml.qnn.TorchLayer(\n            qnode=circuit, weight_shapes=param_shapes, init_method=init_vals\n        )\n\n    def forward(self, x):\n        inputs_stack = torch.hstack([x] * num_reup)\n        return self.qcircuit(inputs_stack)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training the classifier\n\nThe 'test' set will be used as validation in each training step to\nverify the generalization capabilities of our classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#### Hyperparameters ####\nlearning_rate = 0.1\nepochs = 4\nbatch_size = 20\n\n# we use a subset of the training and validation data for this tutorial to speed up the training\nfeats_train = torch.from_numpy(X_train[:200]).reshape(200, -1).to(device)\nfeats_test = torch.from_numpy(X_test[:50]).reshape(50, -1).to(device)\nlabels_train = torch.from_numpy(Y_train[:200]).to(device)\nlabels_test = torch.from_numpy(Y_test[:50]).to(device)\nnum_train = feats_train.shape[0]\n\n# initialize the model, loss function and optimization algorithm (Adam optimizer)\nqml_model = QML_classifier(input_dim, num_classes, num_qubits, num_layers)\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(qml_model.parameters(), lr=learning_rate)\nnum_batches = feats_train.shape[0] // batch_size\n\n\n# set up a measure for performance\ndef accuracy(labels, predictions):\n    acc = 0\n    for l, p in zip(labels, predictions):\n        if torch.argmax(p) == l:\n            acc += 1\n    acc = acc / len(labels)\n    return acc\n\n\n# generate randomly permutated batches to speed up training\ndef gen_batches(num_samples, num_batches):\n    assert num_samples % num_batches == 0\n    perm_ind = torch.reshape(torch.randperm(num_samples), (num_batches, -1))\n    return perm_ind\n\n\n# display accuracy and loss after each epoch (to speed up runtime, only do this for first 100 samples)\ndef print_acc(epoch, max_ep=4):\n    predictions_train = [qml_model(f) for f in feats_train[:50]]\n    predictions_test = [qml_model(f) for f in feats_test]\n    cost_approx_train = loss(torch.stack(predictions_train), labels_train[:50])\n    cost_approx_test = loss(torch.stack(predictions_test), labels_test)\n    acc_approx_train = accuracy(labels_train[:50], predictions_train)\n    acc_approx_test = accuracy(labels_test, predictions_test)\n    print(\n        f\"Epoch {epoch}/{max_ep} | Approx Cost (train): {cost_approx_train:0.7f} | Cost (val): {cost_approx_test:0.7f} |\"\n        f\" Approx Acc train: {acc_approx_train:0.7f} | Acc val: {acc_approx_test:0.7f}\"\n    )\n\n\nprint(\n    f\"Starting training loop for quantum variational classifier ({num_qubits} qubits, {num_layers} layers)...\"\n)\n\n# optimize over model parameters for given number of epochs\nfor ep in range(0, epochs):\n    batch_ind = gen_batches(num_train, num_batches)\n    print_acc(epoch=ep)\n\n    for it in range(num_batches):\n        optimizer.zero_grad()\n        feats_train_batch = feats_train[batch_ind[it]]\n        labels_train_batch = labels_train[batch_ind[it]]\n\n        outputs = [qml_model(f) for f in feats_train_batch]\n        batch_loss = loss(torch.stack(outputs), labels_train_batch)\n        # if REG:\n        #    loss = loss + lipschitz_regularizer(regularization_rate, model.qcircuit.weights)\n        batch_loss.backward()\n        optimizer.step()\n\nprint_acc(epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation - benign data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# show accuracy\nx_vis_torch = torch.from_numpy(np.array(x_vis).reshape(4, -1))\ny_vis_torch = torch.from_numpy(np.array(y_vis))\nbenign_preds = [qml_model(f) for f in x_vis_torch]\n\nbenign_class_output = [torch.argmax(p) for p in benign_preds]\nvisualize_data(x_vis, y_vis, benign_class_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Let's break stuff!\n\nAs described before, the mathematical notation for an adversarial attack\nis as follows:\n\n$$\\delta \\equiv \\; \\underset{\\delta^{\\prime} \\in \\Delta}{\\operatorname{argmax}} \\;\\mathcal{L}\\left(f\\left(x+\\delta^{\\prime} ; \\theta^*\\right), y\\right).$$\n\nThis equation can be summarized in a simple step-by-step recipe. In\nbasic terms, we perform a forward and backward pass through the model\nand loss function (just like we do during training) for the specific\nsamples that we want to find perturbations for. The difference to\ntraining steps is that the gradients we calculate are **with respect to\nthe input features** of the data samples. Using these gradients, we find\nthe direction of ascent in the loss function (as we want to force high\nlosses, i.e.\u00a0bad model performance for the specific samples). Finally,\nwe clip the delta (perturbations), such that they lie within the epsilon\nboundary we set beforehand (this is just some hyperparameter wich limits\nthe attack strength). All of these steps can be seen in the code below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# simple implementation of projected gradient descent (PGD) attack (without randomized starting points \u2014 cf. BIM)\n# for an introduction to PGD, see https://adversarial-ml-tutorial.org/adversarial_examples/#projected-gradient-descent\ndef PGD(model, feats, labels, epsilon=0.1, alpha=0.01, num_iter=10):\n\n    # initialize image perturbations with zero\n    delta = torch.zeros_like(feats, requires_grad=True)\n    for t in range(num_iter):\n        feats_adv = feats + delta\n        outputs = [model(f) for f in feats_adv]\n\n        # forward & backward pass through the model, accumulating gradients\n        l = loss(torch.stack(outputs), labels)\n        l.backward()\n\n        # use gradients with respect to inputs and clip the results to lie within the epsilon boundary\n        delta.data = (delta + alpha * delta.grad.detach().sign()).clamp(-epsilon, epsilon)\n        delta.grad.zero_()\n    return delta.detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation --- model under attack\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "perturbations = PGD(model=qml_model, feats=x_vis_torch, labels=y_vis_torch, epsilon=0.1)\nperturbed_x = x_vis_torch + perturbations\n\n# check model performance\nadversarial_preds = [qml_model(f) for f in perturbed_x]\nadversarial_class_output = [torch.argmax(p) for p in adversarial_preds]\n\nvisualize_data(perturbed_x.reshape(-1, 16, 16), y_vis, adversarial_class_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see the devastating effect of a simple PGD (projected gradient\ndescent) attack using a perturbation strength $\\varepsilon=0.1,$ where\nthe model misclassifies each of the four samples we used for\nvisualization of the dataset. For humans, the images are still very\neasily classifiable, the perturbations look mostly like random noise\nadded to the images. All in all, the accuracy of the model for the\nperturbed trainset decreases to around $0.1,$ so almost all samples of\nthe dataset are misclassified!\n\nUsing the code above, you can try the attack on your own and check which\nsamples are more robust against attacks and which labels the model\n(wrongly) assigns to the data. The remaining question is: Can we defend\nagainst such attacks? How can we train our (QML) models to be more\nrobust against adversarial attacks? This is a field of much active\nresearch and in general a very hard problem to solve for different kinds\nof attacks, model architectures and input data, but a very simple\napproach is given below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Increasing the robustness of QML models\n\nThe easiest way to make our model aware of slight modifications is to\nsimply include the perturbed images into the training dataset (this can\nbe seen as data augmentation and is often done in classical machine\nlearning using noise). That way, the model is expected to be more robust\nagainst samples that were constructed using the same attack without\nchanging anything in the model architecture itself; this method is\ncalled **adversarial (re)training**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "adv_dataset = (\n    PGD(model=qml_model, feats=feats_train[:20], labels=labels_train[:20], epsilon=0.1)\n    + feats_train[:20]\n)\n\nfeats_retrain = torch.cat((feats_train, adv_dataset))\nlabels_retrain = torch.cat((labels_train, labels_train[:20]))\nepochs_retraining = 2\n\nfor ep in range(0, epochs_retraining):\n    batch_ind = gen_batches(num_train, num_batches)\n    print_acc(epoch=ep, max_ep=2)\n\n    for it in range(num_batches):\n        optimizer.zero_grad()\n        feats_train_batch = feats_retrain[batch_ind[it]]\n        labels_train_batch = labels_retrain[batch_ind[it]]\n\n        outputs = [qml_model(f) for f in feats_train_batch]\n        batch_loss = loss(torch.stack(outputs), labels_train_batch)\n        # if REG:\n        #    loss = loss + lipschitz_regularizer(regularization_rate, model.qcircuit.weights)\n        batch_loss.backward()\n        optimizer.step()\n\nprint_acc(epochs_retraining, max_ep=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation of the retrained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "adversarial_preds = [qml_model(f) for f in perturbed_x]\nadversarial_class_output = [torch.argmax(p) for p in adversarial_preds]\n\nvisualize_data(perturbed_x.reshape(-1, 16, 16), y_vis, adversarial_class_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the model now correctly classifies three out of the four\nperturbed input images. As before, you can adapt the code above and test\nthe retrained model for the whole dataset to see how much the accuracy\nunder attack improves overall.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n\nIn this demo we saw how to perform adversarial attacks on quantum\nvariational classification models. The resulting perturbed images (which\n--- for humans --- still closely resemble the original ones) result in\nlarge misclassification rates of the QML models, showing the\nvulnerability of such models to adversarial attacks. Consequently, we\nshowcased one possibility to increase the adversarial robustness using\nadversarial retraining, which led the model under attack to perform\nbetter.\n\nWhile this method does seem useful, it needs to be noted that this is a\nvery basic approach (just include the pertubed images in the trainset,\nduh!) and might not work equally well for a different attack or slightly\nchanged epsilon values. There are approaches (e.g.\u00a0using Lipschitz\nregularization as shown in) that have the potential to be more effective\nin increasing the robustness, but this is still a field of ongoing\nresearch, and many different approaches are waiting to be discovered!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# About the authors\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}